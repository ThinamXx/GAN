{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ],
      "metadata": {
        "id": "_ddjPCeY7X2L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R9IdOXhz6Sbz"
      },
      "outputs": [],
      "source": [
        "#@ INITIALIZATION: \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ],
      "metadata": {
        "id": "KG7uQ-lP7xTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPORTING NECESSARY LIBRARIES AND DEPENDENCIES: \n",
        "from torch.nn import ConvTranspose2d\n",
        "from torch.nn import BatchNorm2d\n",
        "from torch.nn import Conv2d\n",
        "from torch.nn import Linear\n",
        "from torch.nn import LeakyReLU\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Tanh\n",
        "from torch.nn import Sigmoid\n",
        "from torch import flatten\n",
        "from torch import nn\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import transforms\n",
        "from sklearn.utils import shuffle\n",
        "import imutils\n",
        "from imutils import build_montages\n",
        "from torch.optim import Adam\n",
        "from torch.nn import BCELoss\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "import os"
      ],
      "metadata": {
        "id": "4_3GUyeN7sbu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DCGANs**\n",
        "- Deep Convolutional Generative Adversarial Networks (DCGANs) was introduced by Radford et al. in their 2016 paper - *Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks*. DCGANs at that time showed us how to effectively use convolutional techniques with GANs without supervision to create images that are quite similar to those in our dataset. "
      ],
      "metadata": {
        "id": "nMtvOhyjSSfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING GENERATOR MODULE:\n",
        "class Generator(nn.Module):                                                     # Defining Generator Module. \n",
        "    def __init__(self, inputDim=100, outputChannels=1):                         # Initializing Constructor Function. \n",
        "        super(Generator, self).__init__()                                       # Initializing Super Constructor. \n",
        "        self.ct1 = ConvTranspose2d(in_channels=inputDim, out_channels=128, \n",
        "                                   kernel_size=4, stride=2, padding=0, \n",
        "                                   bias=False)                                  # Initializing Transposed Convolution. \n",
        "        self.relu1 = ReLU()                                                     # Initializing RELU Activation. \n",
        "        self.batchNorm1 = BatchNorm2d(128)                                      # Initializing Batch Normalization. \n",
        "        self.ct2 = ConvTranspose2d(in_channels=128, out_channels=64, \n",
        "                                   kernel_size=3, stride=2, padding=1,\n",
        "                                   bias=False)                                  # Adding Transposed Convolution. \n",
        "        self.relu2 = ReLU()                                                     # Adding RELU Activation Function. \n",
        "        self.batchNorm2 = BatchNorm2d(64)                                       # Adding Batch Normalization Layer.\n",
        "        self.ct3 = ConvTranspose2d(in_channels=64, out_channels=32, \n",
        "                                   kernel_size=4, stride=2, padding=1, \n",
        "                                   bias=False)                                  # Adding Transposed Convolution. \n",
        "        self.relu3 = ReLU()                                                     # Adding RELU Activation Function. \n",
        "        self.batchNorm3 = BatchNorm2d(32)                                       # Adding Batch Normalization Layer. \n",
        "        self.ct4 = ConvTranspose2d(in_channels=32,out_channels=outputChannels, \n",
        "                                   kernel_size=4, stride=2, padding=1, \n",
        "                                   bias=False)                                  # Adding Transposed Convolution. \n",
        "        self.tanh = Tanh()                                                      # Adding RELU Activation Function. \n",
        "\n",
        "    def forward(self, x):                                                       # Defining Forward Method. \n",
        "        x = self.ct1(x)                                                         # Transposed Convolution. \n",
        "        x = self.relu1(x)                                                       # RELU Activation Function.\n",
        "        x = self.batchNorm1(x)                                                  # Batch Normalization Layer. \n",
        "        x = self.ct2(x)                                                         # Transposed Convolution. \n",
        "        x = self.relu2(x)                                                       # RELU Activation Function.\n",
        "        x = self.batchNorm2(x)                                                  # Batch Normalization Layer. \n",
        "        x = self.ct3(x)                                                         # Transposed Convolution. \n",
        "        x = self.relu3(x)                                                       # RELU Activation Function.\n",
        "        x = self.batchNorm3(x)                                                  # Batch Normalization Layer. \n",
        "        x = self.ct4(x)                                                         # Transposed Convolution. \n",
        "        output = self.tanh(x)                                                   # Tanh Activation Function. \n",
        "        return output"
      ],
      "metadata": {
        "id": "kAl_isfZ8HW1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DISCRIMINATOR:**\n",
        "- Generator module is going to model random noise into an image. Discriminator takes the image and outputs a single value. "
      ],
      "metadata": {
        "id": "Ap0P5SX6MkNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING DISCRIMINATOR MODULE: \n",
        "class Discriminator(nn.Module):                                         # Defining Discriminator Module. \n",
        "    def __init__(self, depth, alpha=0.2):                               # Initializing Constructor Function. \n",
        "        super(Discriminator, self).__init__()                           # Initializing Super Constructor. \n",
        "        self.conv1 = Conv2d(in_channels=depth, out_channels=32, \n",
        "                            kernel_size=4, stride=2, padding=1)         # Initializing Convolutional Layer. \n",
        "        self.leakyRelu1 = LeakyReLU(alpha, inplace=True)                # Initializing Leaky RELU. \n",
        "        self.conv2 = Conv2d(in_channels=32, out_channels=64, \n",
        "                            kernel_size=4, stride=2, padding=1)         # Adding Convolutional Layer. \n",
        "        self.leakyRelu2 = LeakyReLU(alpha, inplace=True)                # Adding Leaky RELU. \n",
        "        self.fc1 = Linear(in_features=3136, out_features=512)           # Adding Linear FC Layer. \n",
        "        self.leakyRelu3 = LeakyReLU(alpha, inplace=True)                # Adding Leaky RELU. \n",
        "        self.fc2 = Linear(in_features=512, out_features=1)              # Adding Linear Output Layer. \n",
        "        self.sigmoid = Sigmoid()                                        # Adding Sigmoid Layer. \n",
        "    \n",
        "    def forward(self, x):                                               # Defining Forward Method. \n",
        "        x = self.conv1(x)                                               # Adding Convolutional Layer. \n",
        "        x = self.leakyRelu1(x)                                          # Leaky RELU Activation. \n",
        "        x = self.conv2(x)                                               # Adding Convolutional Layer. \n",
        "        x = self.leakyRelu2(x)                                          # Leaky RELU Activation. \n",
        "        x = flatten(x, 1)                                               # Adding Flatten Layer. \n",
        "        x = self.fc1(x)                                                 # Adding Linear Layer. \n",
        "        x = self.leakyRelu3(x)                                          # Leaky RELU Activation. \n",
        "        x = self.fc2(x)                                                 # Linear Output Layer. \n",
        "        output = self.sigmoid(x)                                        # Sigmoid Activation Function. \n",
        "        return output"
      ],
      "metadata": {
        "id": "DJdLAOK2BJ-l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAINING DCGANs**"
      ],
      "metadata": {
        "id": "GzZmPIRbVWHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ CUSTOM WEIGHTS INITIALIZATION FUNCTION: \n",
        "def weights_init(model):                                # Defining Weight Initialization Function. \n",
        "    classname = model.__class__.__name__                # Initializing Model Name.\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        nn.init.normal_(model.weight.data, 0.0, 0.02)   # Initializing Weights for Convolutional Layer.\n",
        "    elif classname.find(\"BatchNorm\") != -1:\n",
        "        nn.init.normal_(model.weight.data, 1.0, 0.02)   # Initializing Weights for Batch Normalization Layer. \n",
        "        nn.init.constant_(model.bias.data, 0)           # Initializing Bias for Batch Normalization Layer. "
      ],
      "metadata": {
        "id": "3VNA8RKwPdig"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING PARAMETERS: \n",
        "NUM_EPOCHS = 20                                                             # Initializing Epoch Size. \n",
        "BATCH_SIZE = 128                                                            # Initializing Batch Size. \n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")     # Initializing GPU. \n",
        "\n",
        "#@ INITIALIZING DATA TRANSFORMATIONS: \n",
        "dataTransforms = transforms.Compose([transforms.ToTensor(),                 # Converting into Tensors. \n",
        "                                     transforms.Normalize((0.5), (0.5))])   # Normalizing the Data. \n",
        "\n",
        "#@ INITIALIZING DATASET: \n",
        "trainData = MNIST(root=\"data\", train=True, download=True, \n",
        "                  transform=dataTransforms)                                 # Initializing Training Data. \n",
        "testData = MNIST(root=\"data\", train=False, download=True,\n",
        "                 transform=dataTransforms)                                  # Initializing Test Data.\n",
        "data = torch.utils.data.ConcatDataset((trainData, testData))                # Stacking the Dataset. \n",
        "\n",
        "#@ INITIALIZING DATALOADERS:\n",
        "dataloader = DataLoader(data, shuffle=True, batch_size=BATCH_SIZE)          # Initializing DataLoader. "
      ],
      "metadata": {
        "id": "s8Q3wGrHXU-4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ TRAINING DCGANs:\n",
        "stepsPerEpoch = len(dataloader.dataset) // BATCH_SIZE                       # Initialization. \n",
        "gen = Generator(inputDim=100, outputChannels=1)                             # Initializing Generator. \n",
        "gen.apply(weights_init)                                                     # Initializing Weights. \n",
        "gen.to(DEVICE)                                                              # Loading into GPU. \n",
        "disc = Discriminator(depth=1)                                               # Initializing Discriminator. \n",
        "disc.apply(weights_init)                                                    # Initializing Weights. \n",
        "disc.to(DEVICE)                                                             # Loading into GPU. \n",
        "\n",
        "#@ INITIALIZING OPTIMIZERS: \n",
        "genOpt = Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999), \n",
        "              weight_decay=0.0002 / NUM_EPOCHS)                             # Generator Optimizer. \n",
        "discOpt = Adam(disc.parameters(), lr=0.0002, betas=(0.5, 0.999), \n",
        "              weight_decay=0.0002 / NUM_EPOCHS)                             # Discriminator Optimizer. \n",
        "criterion = BCELoss()                                                       # Initializing Binary Cross Entropy Loss Function. \n",
        "\n",
        "#@ TRAINING DCGANs:\n",
        "benchmarkNoise = torch.randn(256, 100, 1, 1, device=DEVICE)                 # Generating Noise.\n",
        "realLabel, fakeLabel = 1, 0                                                 # Initialization. \n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(\"Starting epoch {} of {}...\".format(epoch + 1, NUM_EPOCHS))       # Inspecting Epochs. \n",
        "    epochLossG, epochLossD = 0, 0                                           # Initializing Loss for Generator & Discriminator. \n",
        "    for x in dataloader:\n",
        "        disc.zero_grad()                                                    # Zeroing Discriminator Gradients. \n",
        "        images = x[0]                                                       # Getting Images. \n",
        "        images = images.to(DEVICE)                                          # Loading into GPU.\n",
        "        bs = images.size(0)                                                 # Initializing Batch Size. \n",
        "        labels = torch.full((bs,), realLabel, dtype=torch.float, \n",
        "                            device=DEVICE)                                  # Initializing Labels. \n",
        "        output = disc(images).view(-1)                                      # Forward Pass and Reshaping. \n",
        "        errorReal = criterion(output, labels)                               # Calculating Loss. \n",
        "        errorReal.backward()                                                # Calculating Gradients. \n",
        "        noise = torch.randn(bs, 100, 1, 1, device=DEVICE)                   # Initialzing Noise for Generator. \n",
        "        fake = gen(noise)                                                   # Generating Fake Image. \n",
        "        labels.fill_(fakeLabel)\n",
        "        output = disc(fake.detach()).view(-1)                               # Forward Pass and Reshaping. \n",
        "        errorFake = criterion(output, labels)                               # Calculating Loss. \n",
        "        errorFake.backward()                                                # Computing Gradients. \n",
        "        errorD = errorReal + errorFake                                      # Computing Error for Discriminator. \n",
        "        discOpt.step()                                                      # Updating. \n",
        "        gen.zero_grad()                                                     # Zeroing Gradients. \n",
        "        labels.fill_(realLabel)\n",
        "        output = disc(fake).view(-1)                                        # Forward Pass and Reshaping. \n",
        "        errorG = criterion(output, labels)                                  # Computing Loss. \n",
        "        errorG.backward()                                                   # Computing Gradients. \n",
        "        genOpt.step()                                                       # Updating. \n",
        "        epochLossD += errorD\n",
        "        epochLossG += errorG\n",
        "    print(\"Generator Loss: {:.4f}, Discriminator Loss: {:.4f}\".format(\n",
        "        epochLossG / stepsPerEpoch, epochLossD / stepsPerEpoch))\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        gen.eval()                                                          # Generator in Evaluation Phase. \n",
        "        images = gen(benchmarkNoise)                                        # Generating Predictions. \n",
        "        images = images.detach().cpu().numpy().transpose((0, 2, 3, 1))\n",
        "        images = ((images * 127.5) + 127.5).astype(\"uint8\")\n",
        "        images = np.repeat(images, 3, axis=-1)\n",
        "        vis = build_montages(images, (28, 28), (16, 16))[0]                 # Building Montage. \n",
        "        p = os.path.join(\"./data/\", \"epoch_{}.png\".format(\n",
        "            str(epoch + 1).zfill(4)))                                       # Building Patches. \n",
        "        cv2.imwrite(p, vis)\n",
        "        gen.train()                                                         # Generator in Training Mode. "
      ],
      "metadata": {
        "id": "1tZol7azcDbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a8923c8-2163-47e6-d792-97329280510f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1 of 20...\n",
            "Generator Loss: 4.2994, Discriminator Loss: 0.4664\n",
            "Starting epoch 2 of 20...\n",
            "Generator Loss: 1.1703, Discriminator Loss: 1.0551\n",
            "Starting epoch 3 of 20...\n",
            "Generator Loss: 0.9579, Discriminator Loss: 1.1877\n",
            "Starting epoch 4 of 20...\n",
            "Generator Loss: 0.8817, Discriminator Loss: 1.2405\n",
            "Starting epoch 5 of 20...\n",
            "Generator Loss: 0.8739, Discriminator Loss: 1.2506\n",
            "Starting epoch 6 of 20...\n",
            "Generator Loss: 0.8890, Discriminator Loss: 1.2483\n",
            "Starting epoch 7 of 20...\n",
            "Generator Loss: 0.8814, Discriminator Loss: 1.2529\n",
            "Starting epoch 8 of 20...\n",
            "Generator Loss: 0.8886, Discriminator Loss: 1.2565\n",
            "Starting epoch 9 of 20...\n",
            "Generator Loss: 0.8848, Discriminator Loss: 1.2547\n",
            "Starting epoch 10 of 20...\n",
            "Generator Loss: 0.8898, Discriminator Loss: 1.2540\n",
            "Starting epoch 11 of 20...\n",
            "Generator Loss: 0.9032, Discriminator Loss: 1.2445\n",
            "Starting epoch 12 of 20...\n",
            "Generator Loss: 0.9131, Discriminator Loss: 1.2403\n",
            "Starting epoch 13 of 20...\n",
            "Generator Loss: 0.9258, Discriminator Loss: 1.2329\n",
            "Starting epoch 14 of 20...\n",
            "Generator Loss: 0.9372, Discriminator Loss: 1.2203\n",
            "Starting epoch 15 of 20...\n",
            "Generator Loss: 0.9521, Discriminator Loss: 1.2114\n",
            "Starting epoch 16 of 20...\n",
            "Generator Loss: 0.9679, Discriminator Loss: 1.1999\n",
            "Starting epoch 17 of 20...\n",
            "Generator Loss: 0.9872, Discriminator Loss: 1.1869\n",
            "Starting epoch 18 of 20...\n",
            "Generator Loss: 1.0057, Discriminator Loss: 1.1791\n",
            "Starting epoch 19 of 20...\n",
            "Generator Loss: 1.0236, Discriminator Loss: 1.1613\n",
            "Starting epoch 20 of 20...\n",
            "Generator Loss: 1.0494, Discriminator Loss: 1.1493\n"
          ]
        }
      ]
    }
  ]
}