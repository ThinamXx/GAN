{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ],
      "metadata": {
        "id": "_ddjPCeY7X2L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R9IdOXhz6Sbz"
      },
      "outputs": [],
      "source": [
        "#@ INITIALIZATION: \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ],
      "metadata": {
        "id": "KG7uQ-lP7xTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPORTING NECESSARY LIBRARIES AND DEPENDENCIES: \n",
        "from torch.nn import ConvTranspose2d\n",
        "from torch.nn import BatchNorm2d\n",
        "from torch.nn import Conv2d\n",
        "from torch.nn import Linear\n",
        "from torch.nn import LeakyReLU\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Tanh\n",
        "from torch.nn import Sigmoid\n",
        "from torch import flatten\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "4_3GUyeN7sbu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DCGANs**\n",
        "- Deep Convolutional Generative Adversarial Networks (DCGANs) was introduced by Radford et al. in their 2016 paper - *Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks*. DCGANs at that time showed us how to effectively use convolutional techniques with GANs without supervision to create images that are quite similar to those in our dataset. "
      ],
      "metadata": {
        "id": "nMtvOhyjSSfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING GENERATOR MODULE:\n",
        "class Generator(nn.Module):                                                     # Defining Generator Module. \n",
        "    def __init__(self, inputDim=100, outputChannels=1):                         # Initializing Constructor Function. \n",
        "        super(Generator, self).__init__()                                       # Initializing Super Constructor. \n",
        "        self.ct1 = ConvTranspose2d(in_channels=inputDim, out_channels=128, \n",
        "                                   kernel_size=4, stride=2, padding=0, \n",
        "                                   bias=False)                                  # Initializing Transposed Convolution. \n",
        "        self.relu1 = ReLU()                                                     # Initializing RELU Activation. \n",
        "        self.batchNorm1 = BatchNorm2d(128)                                      # Initializing Batch Normalization. \n",
        "        self.ct2 = ConvTranspose2d(in_channels=128, out_channels=64, \n",
        "                                   kernel_size=3, stride=2, padding=1,\n",
        "                                   bias=False)                                  # Adding Transposed Convolution. \n",
        "        self.relu2 = ReLU()                                                     # Adding RELU Activation Function. \n",
        "        self.batchNorm2 = BatchNorm2d(64)                                       # Adding Batch Normalization Layer.\n",
        "        self.ct3 = ConvTranspose2d(in_channels=64, out_channels=32, \n",
        "                                   kernel_size=4, stride=2, padding=1, \n",
        "                                   bias=False)                                  # Adding Transposed Convolution. \n",
        "        self.relu3 = ReLU()                                                     # Adding RELU Activation Function. \n",
        "        self.batchNorm3 = BatchNorm2d(32)                                       # Adding Batch Normalization Layer. \n",
        "        self.ct4 = ConvTranspose2d(in_channels=32,out_channels=outputChannels, \n",
        "                                   kernel_size=4, stride=2, padding=1, \n",
        "                                   bias=False)                                  # Adding Transposed Convolution. \n",
        "        self.tanh = Tanh()                                                      # Adding RELU Activation Function. \n",
        "    \n",
        "    def forward(self, x):                                                       # Defining Forward Method. \n",
        "        x = self.ct1(x)                                                         # Transposed Convolution. \n",
        "        x = self.relu1(x)                                                       # RELU Activation Function.\n",
        "        x = self.batchNorm1(x)                                                  # Batch Normalization Layer. \n",
        "        x = self.ct2(x)                                                         # Transposed Convolution. \n",
        "        x = self.relu2(x)                                                       # RELU Activation Function.\n",
        "        x = self.batchNorm2(x)                                                  # Batch Normalization Layer. \n",
        "        x = self.ct3(x)                                                         # Transposed Convolution. \n",
        "        x = self.relu3(x)                                                       # RELU Activation Function.\n",
        "        x = self.batchNorm3(x)                                                  # Batch Normalization Layer. \n",
        "        x = self.ct4(x)                                                         # Transposed Convolution. \n",
        "        output = self.tanh(x)                                                   # Tanh Activation Function. \n",
        "        return output"
      ],
      "metadata": {
        "id": "kAl_isfZ8HW1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DISCRIMINATOR:**\n",
        "- Generator module is going to model random noise into an image. Discriminator takes the image and outputs a single value. "
      ],
      "metadata": {
        "id": "Ap0P5SX6MkNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING DISCRIMINATOR MODULE: \n",
        "class Discriminator(nn.Module):                                         # Defining Discriminator Module. \n",
        "    def __init__(self, depth, alpha=0.2):                               # Initializing Constructor Function. \n",
        "        super(Discriminator, self).__init__()                           # Initializing Super Constructor. \n",
        "        self.conv1 = Conv2d(in_channels=depth, out_channels=32, \n",
        "                            kernel_size=4, stride=2, padding=1)         # Initializing Convolutional Layer. \n",
        "        self.leakyRelu1 = LeakyReLU(alpha, inplace=True)                # Initializing Leaky RELU. \n",
        "        self.conv2 = Conv2d(in_channels=32, out_channels=64, \n",
        "                            kernel_size=4, stride=2, padding=1)         # Adding Convolutional Layer. \n",
        "        self.leakyRelu2 = LeakyReLU(alpha, inplace=True)                # Adding Leaky RELU. \n",
        "        self.fc1 = Linear(in_features=3136, out_features=512)           # Adding Linear FC Layer. \n",
        "        self.leakyRelu3 = LeakyReLU(alpha, inplace=True)                # Adding Leaky RELU. \n",
        "        self.fc2 = Linear(in_features=512, out_features=1)              # Adding Linear Output Layer. \n",
        "        self.sigmoid = Sigmoid()                                        # Adding Sigmoid Layer. \n",
        "    \n",
        "    def forward(self, x):                                               # Defining Forward Method. \n",
        "        x = self.conv1(x)                                               # Adding Convolutional Layer. \n",
        "        x = self.leakyRelu1(x)                                          # Leaky RELU Activation. \n",
        "        x = self.conv2(x)                                               # Adding Convolutional Layer. \n",
        "        x = self.leakyRelu2(x)                                          # Leaky RELU Activation. \n",
        "        x = flatten(x, 1)                                               # Adding Flatten Layer. \n",
        "        x = self.fc1(x)                                                 # Adding Linear Layer. \n",
        "        x = self.leakyRelu3(x)                                          # Leaky RELU Activation. \n",
        "        x = self.fc2(x)                                                 # Linear Output Layer. \n",
        "        output = self.sigmoid(x)                                        # Sigmoid Activation Function. \n",
        "        return output"
      ],
      "metadata": {
        "id": "DJdLAOK2BJ-l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3VNA8RKwPdig"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}